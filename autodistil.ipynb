{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a3cf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 20 18:26:34 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3060         On | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   56C    P5               14W / 170W|    335MiB / 12288MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A     50326      G   /usr/lib/xorg/Xorg                          154MiB |\r\n",
      "|    0   N/A  N/A     50466      G   /usr/bin/gnome-shell                        139MiB |\r\n",
      "|    0   N/A  N/A     53555      G   ...04393999,4687486099096252267,262144       38MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f83c724",
   "metadata": {},
   "source": [
    "## Autodistill "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd4791",
   "metadata": {},
   "source": [
    "### Base-Model (labeling task) : `DETIC, Grounded SAM, Grounded DINO, OWL-ViT, FastSAM`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd4093",
   "metadata": {},
   "source": [
    "### Target-Model (Detection/ Segmentation) : `yolov8, yolov5, yolonas & DETR`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba59db",
   "metadata": {},
   "source": [
    "### Installing dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "386ab337",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "autodistill \\\n",
    "autodistill-grounded-sam \\\n",
    "autodistill-yolov8 \\\n",
    "supervision==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92aaeeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jay/Gaurav-2\n"
     ]
    }
   ],
   "source": [
    "#Getting the path of working directory\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3148ac0f",
   "metadata": {},
   "source": [
    "### This is the path of our data folder (image folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b676b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR_PATH = f\"{HOME}/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bae787f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image count: 951\n"
     ]
    }
   ],
   "source": [
    "import supervision as sv  # supervision for handling images and videos\n",
    "\n",
    "image_paths = sv.list_files_with_extensions(\n",
    "    directory=IMAGE_DIR_PATH,\n",
    "    extensions=[\"png\", \"jpg\", \"jpeg\"])\n",
    "\n",
    "print('image count:', len(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e4c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR_PATH = f\"{HOME}/images\" \n",
    "SAMPLE_SIZE = 16\n",
    "SAMPLE_GRID_SIZE = (4, 4)\n",
    "SAMPLE_PLOT_SIZE = (16, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7fa6f0",
   "metadata": {},
   "source": [
    "### CaptionOntology is used for defining the label and object of our interest which we want autodistill to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78191b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill.detection import CaptionOntology\n",
    "\n",
    "ontology=CaptionOntology({\n",
    "    \"cardboard box\": \"box\",\n",
    "})\n",
    "## autodistill will label our object(cardboard box) as box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de307723",
   "metadata": {},
   "source": [
    "### Define the path of dataset where we want our labled data with training and testing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e0b802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR_PATH = f\"{HOME}/dataset\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb330337",
   "metadata": {},
   "source": [
    "## We can use diffrent models as per our need for the labling task  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87373393",
   "metadata": {},
   "source": [
    "### Models for labing : `DETIC, Grounded SAM, Grounded DINO, OWL-ViT, FastSAM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e3af8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to load grounding dino directly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/Gaurav-2/venv/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling /home/jay/Gaurav-2/images/box2_jpg.rf.fe7467346f2c0c1b29d5a9738b287914./home/jay/Gaurav-2/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/jay/Gaurav-2/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "Labeling /home/jay/Gaurav-2/images/cardboard1874_jpg.rf.7f44710d0b72721df72d8c8e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset created - ready for distillation.\n"
     ]
    }
   ],
   "source": [
    "from autodistill_grounded_sam import GroundedSAM\n",
    "base_model = GroundedSAM(ontology=ontology)\n",
    "dataset = base_model.label(\n",
    "    input_folder=IMAGE_DIR_PATH,\n",
    "    extension=\".jpg\",\n",
    "    output_folder=DATASET_DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the path where the images, labels and config file (Yaml) file are stored\n",
    "ANNOTATIONS_DIRECTORY_PATH = f\"{HOME}/dataset/train/labels\"\n",
    "IMAGES_DIRECTORY_PATH = f\"{HOME}/dataset/train/images\"\n",
    "DATA_YAML_PATH = f\"{HOME}/dataset/data.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "dataset = sv.DetectionDataset.from_yolo(\n",
    "    images_directory_path=IMAGES_DIRECTORY_PATH,\n",
    "    annotations_directory_path=ANNOTATIONS_DIRECTORY_PATH,\n",
    "    data_yaml_path=DATA_YAML_PATH)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15edf22b",
   "metadata": {},
   "source": [
    "## Plotting the image with labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "image_names = list(dataset.images.keys())[:SAMPLE_SIZE]\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator()\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "\n",
    "images = []\n",
    "for image_name in image_names:\n",
    "    image = dataset.images[image_name]\n",
    "    annotations = dataset.annotations[image_name]\n",
    "    labels = [\n",
    "        dataset.classes[class_id]\n",
    "        for class_id\n",
    "        in annotations.class_id]\n",
    "    annotates_image = mask_annotator.annotate(\n",
    "        scene=image.copy(),\n",
    "        detections=annotations)\n",
    "    annotates_image = box_annotator.annotate(\n",
    "        scene=annotates_image,\n",
    "        detections=annotations,\n",
    "        labels=labels)\n",
    "    images.append(annotates_image)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=images,\n",
    "    titles=image_names,\n",
    "    grid_size=SAMPLE_GRID_SIZE,\n",
    "    size=SAMPLE_PLOT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e59ff",
   "metadata": {},
   "source": [
    "##  Train target model - YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0abac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '{HOME}'\n",
      "/home/jay/Gaurav-2\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd0b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill_yolov8 import YOLOv8\n",
    "\n",
    "target_model = YOLOv8(\"yolov8n.pt\")\n",
    "target_model.train(DATA_YAML_PATH, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {HOME}/runs/detect/train/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b53c2",
   "metadata": {},
   "source": [
    "### Evaluate Target Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=f'{HOME}/runs/detect/train/confusion_matrix.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d686883",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=f'{HOME}/runs/detect/train/results.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2af851",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=f'{HOME}/runs/detect/train/val_batch0_pred.jpg', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2f3a4",
   "metadata": {},
   "source": [
    "### Run Inference on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b7e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_VIDEO_PATH = TEST_VIDEO_PATHS[0]\n",
    "OUTPUT_VIDEO_PATH = f\"{HOME}/output.mp4\"\n",
    "TRAINED_MODEL_PATH = f\"{HOME}/runs/detect/train/weights/best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7773a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo predict model={TRAINED_MODEL_PATH} source={INPUT_VIDEO_PATH}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
